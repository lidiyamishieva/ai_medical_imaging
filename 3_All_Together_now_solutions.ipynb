{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Together Now!\n",
    "\n",
    "We've been going through each individual step by hand, while there's plenty of helpful libraries out there than can do what we've done (and probably better).\n",
    "Let's cover some of the more popular libraries for data manipulation and machine learning in Python.\n",
    "\n",
    "## Quick Note\n",
    "\n",
    "We're using `tqdm` to show progress bars in this notebook.\n",
    "It may not always show nicely, depending on your environment, or preferred ide/editor.\n",
    "It should be purely cosmetic, so it shouldn't affect the code execution.\n",
    "\n",
    "## Lightning Round!\n",
    "\n",
    "[`lightning`](https://lightning.ai/docs/pytorch/stable/), originally PyTorch Lightning, is a python library build on top of `torch` that provides a simplified interface for training models.\n",
    "Whereas previously we've been implementing our own training loops manually, `lightning` provides a lot of the boilerplate code for us.\n",
    "Simply put, it codifies the common steps in training a model, so you can focus on the specifics of your model.\n",
    "\n",
    "For example, here's a simple example of training a mockup model with `lightning`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import lightning as pl\n",
    "from sklearn.utils.model_selection import train_test_split\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_fn=torch.nn.functional.cross_entropy,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, samples, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.samples = samples\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train, test = train_test_split(self.samples, test_size=0.2)\n",
    "        train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(*train)\n",
    "        self.val_dataset = torch.utils.data.TensorDataset(*val)\n",
    "        self.test_dataset = torch.utils.data.TensorDataset(*test)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "model = MyModel()\n",
    "datamodule = MyDataModule(samples)\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "result = trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "This is a very simple example, but it shows how `lightning` can simplify the process of training a model.\n",
    "Note how the `MyModel` class is a subclass of `pl.LightningModule`, but seems very similar to the `nn.Module` we've been using.\n",
    "The primary difference is that `lightning` provides a `[training/validation/testing]_step` method that is called for each batch of data, and a `configure_optimizers` method that returns the optimizer to use.\n",
    "These are the functions that the `pl.Trainer` class uses to train the model.\n",
    "\n",
    "The `MyDataModule` class is a subclass of `pl.LightningDataModule`, and provides a unified interface for loading data.\n",
    "It has a `setup` method that is called before training, and `train/val/test_dataloader` methods that return the appropriate `DataLoader` for each stage of training.\n",
    "This is, again, passed to the `pl.Trainer` class to train the model.\n",
    "\n",
    "Finally, the `pl.Trainer` class is used to train the model.\n",
    "It replaces the manual training loop we've been using, and provides a lot of useful features like early stopping, logging, and checkpointing.\n",
    "\n",
    "Some people are not fans of `lightning`, as it can be a bit heavy-handed and abstract away too much of the details.\n",
    "We'll leave it up to you to decide if it's right for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONAI\n",
    "\n",
    "[`MONAI`](https://monai.io/) is a library for deep learning in medical imaging.\n",
    "It provides a lot of the same functionality as `torch` and `lightning`, but is specifically tailored for medical imaging.\n",
    "In particular, the `monai.data` module provides a lot of useful tools for loading and preprocessing medical imaging data, and the `monai.transforms` module provides a lot of useful tools for preprocessing medical imaging data.\n",
    "A strong point in favour of `monai` is that it provides the dictionary transform paradigm, which allows you to apply a series of transforms to a dictionary of data.\n",
    "\n",
    "What does that mean?\n",
    "\n",
    "Well, let's say you have a dataset of medical images, and you want to apply a series of transforms, or augmentations, to each image.\n",
    "You could manually write a series of functions that take an image as input and return an augmented image as output.\n",
    "Or, you can set up a pipeline of transforms that take a dictionary of data as input and return a dictionary of data as output.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "from monai.data import CacheDataset\n",
    "from monai import transforms as mt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "samples = [\n",
    "    {\"image\": path_to_image1, \"label\": path_to_label1},\n",
    "    {\"image\": path_to_image2, \"label\": path_to_label2},\n",
    "    ...\n",
    "]\n",
    "\n",
    "transforms = mt.Compose([\n",
    "    mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "    mt.Resized(keys=[\"image\", \"label\"], spatial_size=(256, 256)),\n",
    "    mt.RandRotated(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        prob=0.5,\n",
    "        range_x[0, 360],\n",
    "        range_y=[0, 360],\n",
    "        range_z=[0, 360],\n",
    "        mode=[\"bilinear\", \"nearest\"]\n",
    "    ),\n",
    "    mt.ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "dataset = CacheDataset(data=samples, transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "...\n",
    "```\n",
    "\n",
    "Note how the transforms can be composed into the `mt.Compose` transform, and then applied to the dataset.\n",
    "This way you set up your pipeline once, including all of the paramaters for each transform, and then apply it to the dataset.\n",
    "\n",
    "The `CacheDataset` class is a subclass of `torch.utils.data.Dataset`, and has a `transform` parameter that takes a series of transforms to apply to the data.\n",
    "What sets the `CacheDataset` apart from the naive `torch` `Dataset` is that it caches the non-randomizable transformed data, so you only have to transform it once.\n",
    "In our example code, for example, it would run the transforms up to the `Resized` transform once, and then cache the result.\n",
    "In subsequent epochs, it would only run the `RandRotated` and `ToTensord` transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability\n",
    "\n",
    "The presented libraries are not mutually exclusive, and can be used together.\n",
    "However, they are also not nessecarily built to work together, so you may need to do some work to get them to play nice.\n",
    "\n",
    "Let's go through the training of a toy model with the Medical Decathlon Spleen segmentation dataset.\n",
    "The first version will be pure `torch`, the second version will use `monai` for data loading and preprocessing, and the third version will use `lightning` for training.\n",
    "\n",
    "Let's download the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def download_medical_decathlon(url: str, path: Path):\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    data_size = int(response.headers.get('Content-Length', 0))\n",
    "    block_size = 1024\n",
    "\n",
    "    with tqdm(total=data_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "        with open(path, 'wb') as f:\n",
    "            for data in response.iter_content(block_size):\n",
    "                f.write(data)\n",
    "                pbar.update(len(data))\n",
    "\n",
    "def extract_tar(path: Path, dest: Path):\n",
    "    with tarfile.open(path, 'r') as tar:\n",
    "        tar.extractall(dest)\n",
    "\n",
    "\n",
    "url = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "path = Path(r\"./Data/Task09_Spleen.tar\").resolve()\n",
    "dest = Path(r\"./Data/Task09_Spleen\").resolve()\n",
    "if not dest.exists():\n",
    "    dest.mkdir(parents=True)\n",
    "    download_medical_decathlon(url, path)\n",
    "    extract_tar(path, dest)\n",
    "else:\n",
    "    print(f\"{dest} already exists, skipping download and extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll tackle the model architecture.\n",
    "We'll be using a [U-Net](https://arxiv.org/abs/1505.04597), this architecture has shot to fame since its introduction in 2015, forming the basis for many publications since.\n",
    "While there's been a few variations, such as U-Net++, U-Net3+, and others, the basic form of U-Net has still been performing well to this day.\n",
    "The very popular [nnUNet](https://www.nature.com/articles/s41592-020-01008-z), for example, still mainly uses the basic U-Net architecture.\n",
    "\n",
    "We define our model based on the original paper, with some slight alterations to keep things simple.\n",
    "First up, the double convolutions.\n",
    "\n",
    "As per the article:\n",
    "\n",
    "> The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.\n",
    "\n",
    "We'll don't follow the exact specificiations, as we do use padding, to keep the spatial dimensions the same.\n",
    "We can implement this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_normalization: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # This is a little Python trick to make the code more readable\n",
    "        # You can set a variable to contain a function or class, and then call it later\n",
    "        conv = None\n",
    "        norm = None\n",
    "\n",
    "        if n_dims == 2:\n",
    "            conv = nn.Conv2d\n",
    "            norm = nn.BatchNorm2d if use_normalization else nn.Identity\n",
    "        elif n_dims == 3:\n",
    "            conv = nn.Conv3d\n",
    "            norm = nn.BatchNorm3d if use_normalization else nn.Identity\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of dimensions\")\n",
    "        \n",
    "        layers = [\n",
    "            conv(in_channels, out_channels, kernel_size=3, padding=1), # For kernel_size=3, padding=1, and the default stride=1, the output size equals the input size.        \n",
    "            norm(out_channels),                                        # Does Normalization change the dimension of the image? \n",
    "            nn.ReLU(inplace=True),                                     # Does a ReLu activation layer change the dimension of the image?     \n",
    "            conv(out_channels, out_channels, kernel_size=3, padding=1),                                         \n",
    "            norm(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # This is a Python specific syntax to \"unpack\" the list of layers\n",
    "        # and pass them as arguments to nn.Sequential\n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_normalization: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        pool = None\n",
    "        if n_dims == 2:\n",
    "            pool = nn.MaxPool2d\n",
    "        elif n_dims == 3:\n",
    "            pool = nn.MaxPool3d\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of dimensions\")\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            pool(kernel_size=2, stride=2),\n",
    "            DoubleConv(n_dims, in_channels, out_channels, use_normalization),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the decoder part of the network.\n",
    "\n",
    "As per the article:\n",
    "> Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution.\n",
    "\n",
    "Again, we pad the convolutions to keep the spatial dimensions the same, so we don't need to crop the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_transpose: bool = False,\n",
    "        use_normalization: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Our earlier trick to make the code more readable\n",
    "        # Unfortunately doesn't work quite as nicely here because \n",
    "        # nn.Upsample and nn.ConvTranspose have different signatures\n",
    "        conv = None\n",
    "\n",
    "        if n_dims == 2:\n",
    "            conv = nn.Conv2d\n",
    "\n",
    "            # Two methods to upsample: transpose or with interpolation\n",
    "            ## Upsample the spatial dimensions (height, width) and reduce the number of channels by half.\n",
    "            if use_transpose:\n",
    "                self.upsample = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)  # We cut the number of in-channels in half. How this operation affects the other dimensions?\n",
    "            else:\n",
    "                self.upsample = nn.Sequential(  # We want to the same image dimension as by using use_transpose\n",
    "                    nn.Upsample(   # What is it changing here? \n",
    "                        scale_factor=2,\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=True\n",
    "                    ),\n",
    "                    conv(in_channels, in_channels // 2, kernel_size=1, padding=0),  # We performe a 1x1 convolution. Why is this useful? Did the image change shape?\n",
    "                )\n",
    "        elif n_dims == 3:\n",
    "            conv = nn.Conv3d\n",
    "\n",
    "            if use_transpose:\n",
    "                self.upsample = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            else:\n",
    "                self.upsample = nn.Sequential(\n",
    "                    nn.Upsample(\n",
    "                        scale_factor=2,\n",
    "                        mode=\"trilinear\",  # Why do we use trilinear interpolation here and not bilinear? What is the difference?\n",
    "                        align_corners=True\n",
    "                    ),\n",
    "                    conv(in_channels, in_channels // 2, kernel_size=1, padding=0),\n",
    "                )\n",
    "        \n",
    "        self.decode = DoubleConv(n_dims, in_channels, out_channels, use_normalization)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # This is the skip connection\n",
    "        x = torch.cat((x, skip), dim=1)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together in the U-Net architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        base_channels: int = 8,\n",
    "        depth: int = 4,\n",
    "        use_transpose: bool = False,\n",
    "        use_normalization: bool = True,\n",
    "        final_activation: nn.Module | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_dims = n_dims\n",
    "        self.depth = depth\n",
    "\n",
    "        if depth < 2:\n",
    "            raise ValueError(\"Model depth must be 2 or greater\")\n",
    "        \n",
    "        # Define the input layer\n",
    "        layers = [DoubleConv(n_dims, in_channels, base_channels, use_normalization)]\n",
    "        \n",
    "        # Define the encoder path: it progressively doubles the number of channels\n",
    "        current_features = base_channels\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(EncoderBlock(n_dims, current_features, current_features * 2, use_normalization))\n",
    "            current_features *= 2\n",
    "\n",
    "        # Define the decoder path: progressively halves the number of channels\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(DecoderBlock(n_dims, current_features, current_features // 2, use_transpose, use_normalization))\n",
    "            current_features //= 2\n",
    "        \n",
    "        # Define the output layer\n",
    "        if n_dims == 2:\n",
    "            layers.append(nn.Conv2d(current_features, out_channels, kernel_size=1))\n",
    "        elif n_dims == 3:\n",
    "            layers.append(nn.Conv3d(current_features, out_channels, kernel_size=1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of dimensions\")\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        if final_activation is not None:\n",
    "            self.final_activation = final_activation\n",
    "        else:\n",
    "            self.final_activation = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        xi = [self.layers[0](x)]\n",
    "\n",
    "        # Encoder path\n",
    "        # Pretty simple, just loop over the encoder blocks\n",
    "        for layer in self.layers[1:self.depth]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        \n",
    "        # Decoder path\n",
    "        # We need to loop over the decoder blocks, but also need to\n",
    "        # keep track of the skip connections\n",
    "        for i, layer in enumerate(self.layers[self.depth:-1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        \n",
    "        return self.final_activation(self.layers[-1](xi[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "Now we can move on to the data loading and preprocessing.\n",
    "\n",
    "Let's define the preprocessing pipeline first.\n",
    "We'll keep it very simple here, just normalizing the intensities and a random crop.\n",
    "\n",
    "Note that for testing you'll want a deterministic pipeline, so the random crop should be turned off.\n",
    "For simplicity, we'll just use the same pipeline for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    image: torch.Tensor,\n",
    "    label: torch.Tensor | None = None,\n",
    "    crop_size: tuple[int, ...] = (28, 28, 28)\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Normalize the image (Z-score normalization)\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    \n",
    "    # Add a channel dimension to the image\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Random crop\n",
    "    crop_origin = [0, 0, 0]\n",
    "    for dim in range(3):  # Remember, image.shape = [Ch, X, Y, Z ]\n",
    "        max_value = image.shape[...] - crop_size[...]\n",
    "        crop_origin[dim] = torch.randint(0, max_value, (1,)).item()\n",
    "    \n",
    "    image = image[\n",
    "        ...,\n",
    "        crop_origin[0]:crop_origin[0] + crop_size[0],\n",
    "        crop_origin[1]:crop_origin[1] + crop_size[1],\n",
    "        crop_origin[2]:crop_origin[2] + crop_size[2],\n",
    "    ]\n",
    "    \n",
    "    # Add a channel dimension to the label\n",
    "    if label is not None:\n",
    "        label = label.unsqueeze(0)\n",
    "\n",
    "        label = label[\n",
    "            ...,\n",
    "            crop_origin[0]:crop_origin[0] + crop_size[0],\n",
    "            crop_origin[1]:crop_origin[1] + crop_size[1],\n",
    "            crop_origin[2]:crop_origin[2] + crop_size[2],\n",
    "        ]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using the Medical Decathlon Spleen segmentation dataset, we'll need to define a custom `Dataset` class to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def collect_samples(root: Path, test_set: bool = False) -> list[tuple[Path, ...]]:\n",
    "    \"\"\"\n",
    "    Collects the samples from the Medical Decathlon dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : Path\n",
    "        The root directory of the dataset.\n",
    "    test_set : bool\n",
    "        Whether to collect the test set or the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[Path, ...]]\n",
    "        A list of tuples containing the image and label paths.\n",
    "    \"\"\"\n",
    "    # We need to find the files in the dataset\n",
    "    # The dataset is structured as follows:\n",
    "    # root\n",
    "    # └── Task09_Spleen\n",
    "    #     ├── imagesTr\n",
    "    #     │   ├── spleen_1.nii.gz\n",
    "    #     │   ├── spleen_2.nii.gz\n",
    "    #     │   └── ...\n",
    "    #     └── labelsTr\n",
    "    #         ├── spleen_1.nii.gz\n",
    "    #         ├── spleen_2.nii.gz\n",
    "    #         └── ...\n",
    "\n",
    "    if test_set:\n",
    "        image_dir = root / \"imagesTs\"\n",
    "        label_dir = None\n",
    "    else:\n",
    "        image_dir = root / \"imagesTr\"\n",
    "        label_dir = root / \"labelsTr\"\n",
    "\n",
    "    if not image_dir.exists():\n",
    "        raise ValueError(f\"Could not find dataset in {root}\")\n",
    "    \n",
    "    samples = []\n",
    "    images = [x for x in image_dir.iterdir() if x.is_file()]\n",
    "    for image in images:\n",
    "        label = None\n",
    "        if label_dir is not None:\n",
    "            label = label_dir / image.name\n",
    "        \n",
    "        samples.append((image, label))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "class MedicalDecathlonDataset(Dataset):\n",
    "    def __init__(self, samples: list[tuple[Path, ...]], test: bool = False) -> None:\n",
    "        self.samples = samples\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n",
    "        image_path, label_path = self.samples[idx]\n",
    "        \n",
    "        image = sitk.ReadImage(image_path)\n",
    "        image = torch.tensor(sitk.GetArrayFromImage(image)).float() # Convert image to PyTorch tensor and cast it to float\n",
    "\n",
    "        if label_path is None:\n",
    "           image, _ = preprocess(image)\n",
    "           return image\n",
    "\n",
    "        label = sitk.ReadImage(label_path)\n",
    "        label = torch.tensor(sitk.GetArrayFromImage(label)).float()\n",
    "\n",
    "        return preprocess(image, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do our old reliable training loop.\n",
    "\n",
    "This might take a while, we didn't do much to really speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(n_dims=3, in_channels=1, out_channels=1, depth=3).to(device)  # 3D U-Net. How many channels?\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "root = Path(r\"./Data/Task09_Spleen/Task09_Spleen/\").resolve()\n",
    "samples = collect_samples(root)\n",
    "\n",
    "# We're just doing a naive split here, you might want to do something more sophisticated\n",
    "train_samples = samples[:int(len(samples) * 0.8)]  # from 0 to X\n",
    "val_samples = samples[int(len(samples) * 0.8):]    # From X to the end\n",
    "\n",
    "train_ds = MedicalDecathlonDataset(train_samples)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_ds = MedicalDecathlonDataset(val_samples)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in (prog_bar := tqdm(range(n_epochs), desc=\"Training\", unit=\"epoch\", total=n_epochs, position=0)):\n",
    "\n",
    "    model.train() # We set the model in training mode\n",
    "    train_losses = []   # List to track training losses\n",
    "    prog_bar.set_description(f\"Training Loop\")\n",
    "\n",
    "    for i, (image, label) in tqdm(enumerate(train_dl), total=len(train_dl), desc=\"Training\", unit=\"batch\", position=1, leave=False):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        output = model(image) # Model forward pass\n",
    "        loss = loss_fn(output, label) # Compute loss\n",
    "        loss.backward() # Backpropagate loss\n",
    "        optimizer.step() # Update model weights\n",
    "\n",
    "        train_losses.append(loss.item()) # Append training loss for this batch\n",
    "    \n",
    "    prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses)})\n",
    "\n",
    "    prog_bar.set_description(f\"Validation Loop\")\n",
    "     \n",
    "    model.eval() # We set the model in evaluation mode\n",
    "    val_losses = [] \n",
    "    for i, (image, label) in tqdm(enumerate(val_dl), total=len(val_dl), desc=\"Validation\", unit=\"batch\", position=1, leave=False):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "    \n",
    "    prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses), \"Validation loss\": sum(val_losses) / len(val_losses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monai Integration\n",
    "\n",
    "Now that we've trained a model using nothing but base `torch`, let's see how we can improve things with `monai`.\n",
    "\n",
    "First, let's redefine our sample collection code to be more `monai`-friendly.\n",
    "Because we want to use the dictionary transforms, our samples need to be dictionaries with keys for each data item.\n",
    "So, we'll define a function to load the data and return a dictionary.\n",
    "\n",
    "In our implementation, we'll standardize on the keys `\"image\"` and `\"label\"` for the image and label data, respectively.\n",
    "Though you can use any keys you like, as long as they match the keys in the transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def collect_samples(root: Path, is_test: bool = False) -> list[dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Collects the samples from the Medical Decathlon dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : Path\n",
    "        The root directory of the dataset.\n",
    "    is_test : bool\n",
    "        Whether to collect the test set or the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Path]]\n",
    "        A list of dictionaries containing the image and label paths.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_test:\n",
    "        image_dir = root / \"imagesTs\"\n",
    "        label_dir = None\n",
    "    else:\n",
    "        image_dir = root / \"imagesTr\"\n",
    "        label_dir = root / \"labelsTr\"\n",
    "\n",
    "    if not image_dir.exists():\n",
    "        raise ValueError(f\"Could not find dataset in {root}\")\n",
    "    \n",
    "    samples = []\n",
    "    images = [x for x in image_dir.iterdir() if x.is_file()]\n",
    "    for image in images:\n",
    "        sample = {\"image\": image}\n",
    "        if label_dir is not None:\n",
    "            sample[\"label\"] = label_dir / image.name\n",
    "        \n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the transforms.\n",
    "\n",
    "This is pretty straightforward, you define each step sequentially, and provide the transform with the keys it should operate on.\n",
    "Let's also take the opportunity to define a separate pipeline for training and inference.\n",
    "\n",
    "Finally, we pass our transforms to a `CacheDataset`, which will apply the transforms to the data and cache the non-random results.\n",
    "Note that the `CacheDataset` might take a while to run the first time, as it applies all of the transforms to the data.\n",
    "Additionally, your laptop might not have enough memory to cache all of the data.\n",
    "If that is the case, you can also import the `Dataset` class from `monai.data` and use that instead.\n",
    "Be aware that this is _not_ the same as the `torch` `Dataset` class, but is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai import transforms as mt\n",
    "from monai.data import CacheDataset\n",
    "\n",
    "train_transforms = mt.Compose([\n",
    "    mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "    mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[28, 28, 28]),\n",
    "    # You can add more transforms here! See\n",
    "    # https://docs.monai.io/en/stable/transforms.html#dictionary-transforms\n",
    "])\n",
    "\n",
    "val_transforms = mt.Compose([\n",
    "    mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "])\n",
    "\n",
    "root = Path(r\"./Data/Task09_Spleen/Task09_Spleen/\").resolve()\n",
    "samples = collect_samples(root)\n",
    "\n",
    "train_samples = samples[:int(len(samples) * 0.8)]\n",
    "val_samples = samples[int(len(samples) * 0.8):]\n",
    "\n",
    "# You might get an error here, make sure you install the required extra dependencies\n",
    "# https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
    "train_ds = CacheDataset(data=train_samples, transform=train_transforms)\n",
    "val_ds = CacheDataset(data=val_samples, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, it's just a matter of training the model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 4\n",
    "sw_batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(\n",
    "    n_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    depth=3,\n",
    "    final_activation=nn.Sigmoid(),\n",
    ").to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We use the CacheDatasets here\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# We're doing a batch size 1 here because we're using sliding window inference\n",
    "# and not all images are the same size, which is a requirement for batching\n",
    "val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "for epoch in (prog_bar := tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\", total=n_epochs, position=0)):\n",
    "    prog_bar.set_description(\"Training Loop\")\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i, batch in tqdm(enumerate(train_dl), total=len(train_dl), desc=\"Training\", unit=\"batch\", position=1, leave=False):\n",
    "        # Note that we're using the batch dictionary here\n",
    "        # Make sure to adjust the keys if you changed them\n",
    "        image = batch[\"image\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses)})\n",
    "\n",
    "    prog_bar.set_description(\"Validation Loop\")\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for i, batch in tqdm(enumerate(val_dl), total=len(val_dl), desc=\"Validation\", unit=\"batch\", position=1, leave=False):\n",
    "        image = batch[\"image\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # We're using the sliding window inference here\n",
    "            # This is a simple way to handle larger images\n",
    "            # without having to define a custom collate function\n",
    "            output = sliding_window_inference(\n",
    "                image,\n",
    "                (28, 28, 28),\n",
    "                sw_batch_size,\n",
    "                model,\n",
    "                overlap=0.0,  # No overlap between patches\n",
    "                mode=\"constant\",  # Overlap merging strategy\n",
    "                progress=True,\n",
    "            )\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "    \n",
    "    prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses), \"Validation loss\": sum(val_losses) / len(val_losses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet!\n",
    "Note how the training section is also much faster now, as the data is preprocessed and cached.\n",
    "Overall the total time to train will probably be a bit longer, as we're doing a full prediction on the validation set after each epoch instead of just grabbing a small patch.\n",
    "\n",
    "# Lightning Integration\n",
    "\n",
    "Now, finally, we will include `lightning` into the mix.\n",
    "While it is not specifically designed to work with `monai`, them both being built on top of `torch` means that combining the two is relatively straightforward.\n",
    "\n",
    "First up, let's take our model, and convert it to a `lightning` model.\n",
    "There's a few ways we could tackle this.\n",
    "We could encapsulate, or wrap, our model as an element of a `lightning.LightningModule` class.\n",
    "We could also rewrite our original `UNet` class to inherit from `lightning.LightningModule`.\n",
    "Alternatively, we could inherit from both `UNet` and `lightning.LightningModule`, but that might be a bit overkill.\n",
    "\n",
    "For simplicity, we'll go with the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl is the old way to shorten pytorch_lightning\n",
    "# I'm using it here because it is what I'm used to\n",
    "# But the new way is to\n",
    "# import lightning as L\n",
    "import lightning as pl\n",
    "\n",
    "\n",
    "class LightningUNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        base_channels: int = 8,\n",
    "        depth: int = 4,\n",
    "        use_transpose: bool = False,\n",
    "        use_normalization: bool = True,\n",
    "        final_activation: nn.Module | None = None,\n",
    "        input_size: tuple[int, ...] = (28, 28, 28),\n",
    "        lr: float = 1e-3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if n_dims not in (2, 3):\n",
    "            raise ValueError(\"Invalid number of dimensions\")\n",
    "        \n",
    "        if n_dims != len(input_size):\n",
    "            raise ValueError(\"Input size must match number of dimensions\")\n",
    "\n",
    "        # This is for logging hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        # This is used in the ModelSummary callback, not mandatory but nice to have\n",
    "        self.example_input_array = torch.rand(1, in_channels, *input_size)\n",
    "\n",
    "        self.model = UNet(\n",
    "            n_dims=n_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            base_channels=base_channels,\n",
    "            depth=depth,\n",
    "            use_transpose=use_transpose,\n",
    "            use_normalization=use_normalization,\n",
    "            final_activation=final_activation,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.lr = lr\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> dict[str, torch.Tensor]:\n",
    "        # Note how we are using the monai batch dictionary here\n",
    "        # If you weren't using monai, you would have to adjust this\n",
    "        # to match your own batch structure\n",
    "        image = batch[\"image\"]\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        output = self(image)\n",
    "        loss = self.loss_fn(output, label)\n",
    "\n",
    "        log = {\n",
    "            \"train_loss\": loss,\n",
    "            # You can log more things here, for example metrics\n",
    "        }\n",
    "\n",
    "        # self.log() and self.log_dict() are used for making sure\n",
    "        # that the current progress is displayed properly, and written\n",
    "        # by the TensorboardLogger. the use of either is not mandatory\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True, on_step=True)\n",
    "\n",
    "        # training_step() has to return either:\n",
    "        #   - A scalar of the loss\n",
    "        #   - A dictionary, with the key \"loss\" present\n",
    "        # this return is used by Lightning internally\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        image = batch[\"image\"]\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        output = sliding_window_inference(\n",
    "            image,\n",
    "            self.input_size,\n",
    "            sw_batch_size,\n",
    "            model,\n",
    "            overlap=0.0,  # No overlap between patches\n",
    "            mode=\"constant\",  # Overlap merging strategy\n",
    "        )\n",
    "        loss = self.loss_fn(output, label)\n",
    "\n",
    "        log = {\n",
    "            \"val_loss\": loss,\n",
    "            # You can log more things here, for example metrics\n",
    "        }\n",
    "\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        # validation_step() does _not_ require a return,\n",
    "        # in fact, you don't even need to use the same\n",
    "        # loss function as you do in training_step().\n",
    "\n",
    "    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        # This is pretty much the same as validation_step\n",
    "        # but is used for the test set\n",
    "\n",
    "        image = batch[\"image\"]\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        output = sliding_window_inference(\n",
    "            image,\n",
    "            self.input_size,\n",
    "            sw_batch_size,\n",
    "            model,\n",
    "            overlap=0.0,  # No overlap between patches\n",
    "            mode=\"constant\",  # Overlap merging strategy\n",
    "        )\n",
    "        loss = self.loss_fn(output, label)\n",
    "\n",
    "        log = {\n",
    "            \"test_loss\": loss,\n",
    "            # You can log more things here, for example metrics\n",
    "        }\n",
    "\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a `DataModule` class to handle the data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecathlonDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Path,\n",
    "        batch_size: int = 4,\n",
    "        split: float = 0.8,\n",
    "        patch_size: tuple[int, ...] = (28, 28, 28),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.split = split\n",
    "\n",
    "        self.train_set = None\n",
    "        self.val_set = None\n",
    "        self.test_set = None\n",
    "\n",
    "        self.train_transforms = mt.Compose([\n",
    "            mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            mt.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "            mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=patch_size),\n",
    "        ])\n",
    "\n",
    "        # We're turning on allow_missing_keys here\n",
    "        # This is because our toy test set doesn't have labels and we want to be able to run\n",
    "        # the training.\n",
    "        self.val_transforms = mt.Compose([\n",
    "            mt.LoadImaged(keys=[\"image\", \"label\"], allow_missing_keys=True),\n",
    "            mt.EnsureChannelFirstd(keys=[\"image\", \"label\"], allow_missing_keys=True),\n",
    "            mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage: str | None = None) -> None:\n",
    "        samples = collect_samples(self.root)\n",
    "        train_samples = samples[:int(len(samples) * self.split)]\n",
    "        val_samples = samples[int(len(samples) * self.split):]\n",
    "        test_samples = collect_samples(self.root, is_test=True)\n",
    "\n",
    "        self.train_set = CacheDataset(data=train_samples, transform=train_transforms)\n",
    "        self.val_set = CacheDataset(data=val_samples, transform=val_transforms)\n",
    "        self.test_set = CacheDataset(data=test_samples, transform=val_transforms)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_set, batch_size=1, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's just a matter of training the model.\n",
    "\n",
    "Lightning has the concept of a `Trainer` class, which is used to train the model.\n",
    "It handles all of the boilerplate code for training, including logging, checkpointing, and early stopping.\n",
    "Let's train a model using the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import callbacks as plc\n",
    "from lightning.pytorch import loggers\n",
    "\n",
    "patch_size = (28, 28, 28)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    callbacks=[\n",
    "        plc.ModelSummary(max_depth=3),\n",
    "        plc.EarlyStopping(monitor=\"val_loss\"),\n",
    "    ],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "model = LightningUNet(\n",
    "    n_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    base_channels=8,\n",
    "    depth=3,\n",
    "    final_activation=nn.Sigmoid(),\n",
    "    lr=1e-3,\n",
    "    input_size=patch_size,\n",
    ")\n",
    "datamodule = DecathlonDataModule(\n",
    "    root=root,\n",
    "    batch_size=4,\n",
    "    patch_size=patch_size,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for completeness, let's plot a prediction.\n",
    "Don't expect too much performance, as we're only training for a few epochs, and we've not done much in the way of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_set = datamodule.test_dataloader()\n",
    "test_set = iter(test_set)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "batch = next(test_set)\n",
    "image = batch[\"image\"]\n",
    "\n",
    "output = sliding_window_inference(\n",
    "    image,\n",
    "    model.input_size,\n",
    "    sw_batch_size,\n",
    "    model,\n",
    "    overlap=0.0,  # No overlap between patches\n",
    "    mode=\"constant\",  # Overlap merging strategy\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "image = image.squeeze().numpy()\n",
    "output = output.detach().squeeze().numpy()\n",
    "\n",
    "middle_slice = image.shape[-1] // 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(image[..., middle_slice], cmap=\"gray\")\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(output[..., middle_slice], cmap=\"gray\")\n",
    "ax[1].set_title(\"Output\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "We've trained a model using `lightning` and `monai` together.\n",
    "For your own assignments, you can decide which library to use, or mix and match as you see fit.\n",
    "\n",
    "Also consider that there are many other libraries out there that can help you with your machine learning projects.\n",
    "For example, `torchmetrics` might have some more advanced metrics, `monai` itself has a library of common models, losses, and metrics, and `pytorch-lightning` has a lot of advanced features like distributed training and logging.\n",
    "\n",
    "Good luck with your projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
